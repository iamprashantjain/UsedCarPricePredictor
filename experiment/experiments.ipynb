{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "797d0f1e",
   "metadata": {},
   "source": [
    "### import and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a37abfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import dagshub\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import yaml\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cba55601",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(r\"cars24_v3.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66555fd",
   "metadata": {},
   "source": [
    "### mlflow-dagshub setup\n",
    "- read details from params & os env var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fde27d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Now you can access your DAGSUB_PAT\n",
    "DAGSHUB_PAT = os.getenv(\"DAGSHUB_PAT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31d2ce2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load parameters from params.yaml\n",
    "with open(r\"I:\\CampusX_DS\\campusx_dsmp2\\9. MLOps revisited\\tutorial\\UsedCarPricePredictor\\params.yaml\", \"r\") as f:\n",
    "    params = yaml.safe_load(f)\n",
    "\n",
    "repo_owner = params[\"mlflow\"][\"repo_owner\"]\n",
    "repo_name = params[\"mlflow\"][\"repo_name\"]\n",
    "\n",
    "# Set up DagsHub credentials for MLflow tracking\n",
    "dagshub_token = os.getenv(\"DAGSHUB_PAT\")\n",
    "if not dagshub_token:\n",
    "    raise EnvironmentError(\"DAGSHUB_PAT environment variable is not set\")\n",
    "\n",
    "os.environ[\"MLFLOW_TRACKING_USERNAME\"] = dagshub_token\n",
    "os.environ[\"MLFLOW_TRACKING_PASSWORD\"] = dagshub_token\n",
    "\n",
    "dagshub_url = \"https://dagshub.com\"\n",
    "mlflow.set_tracking_uri(f'{dagshub_url}/{repo_owner}/{repo_name}.mlflow')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9181e588",
   "metadata": {},
   "source": [
    "### Exp 1: find best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67c1a768",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "i:\\CampusX_DS\\campusx_dsmp2\\9. MLOps revisited\\tutorial\\UsedCarPricePredictor\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: Linear Regression\n",
      "  MAE: 40516553969.24, MSE: 64350810197141720399872.00, R2: -208443009422.29\n",
      "----------------------------------------\n",
      "\n",
      "Model: Ridge\n",
      "  MAE: 110939.74, MSE: 38489205335.46, R2: 0.88\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "i:\\CampusX_DS\\campusx_dsmp2\\9. MLOps revisited\\tutorial\\UsedCarPricePredictor\\venv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.866e+13, tolerance: 3.130e+11\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: Lasso\n",
      "  MAE: 100750.06, MSE: 57869339670.66, R2: 0.81\n",
      "----------------------------------------\n",
      "\n",
      "Model: Decision Tree\n",
      "  MAE: 143857.90, MSE: 101028675344.09, R2: 0.67\n",
      "----------------------------------------\n",
      "\n",
      "Model: Random Forest\n",
      "  MAE: 116033.47, MSE: 79768242115.56, R2: 0.74\n",
      "----------------------------------------\n",
      "\n",
      "Model: XGBoost\n",
      "  MAE: 112777.99, MSE: 56937918980.86, R2: 0.82\n",
      "----------------------------------------\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002450 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 753\n",
      "[LightGBM] [Info] Number of data points in the train set: 5912, number of used features: 159\n",
      "[LightGBM] [Info] Start training from score 697057.494756\n",
      "\n",
      "Model: LightGBM\n",
      "  MAE: 146777.97, MSE: 113607140582.79, R2: 0.63\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- models ---\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge': Ridge(),\n",
    "    'Lasso': Lasso(),\n",
    "    'Decision Tree': DecisionTreeRegressor(),\n",
    "    'Random Forest': RandomForestRegressor(),\n",
    "    'XGBoost': XGBRegressor(),\n",
    "    'LightGBM': LGBMRegressor()\n",
    "}\n",
    "\n",
    "# --- Preprocessing ---\n",
    "numerical_cols = ['listingPrice', 'odometer', 'fitnessAge','featureCount']\n",
    "categorical_cols = ['make', 'model', 'variant', 'year', 'transmissionType', 'bodyType', 'fuelType','ownership', 'color']\n",
    "\n",
    "# Impute missing values\n",
    "numerical_imputer = SimpleImputer(strategy='mean')\n",
    "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "df[numerical_cols] = numerical_imputer.fit_transform(df[numerical_cols])\n",
    "df[categorical_cols] = categorical_imputer.fit_transform(df[categorical_cols])\n",
    "\n",
    "# Encode categorical\n",
    "encoder = OneHotEncoder(sparse=False, drop='first')\n",
    "encoded_categorical = encoder.fit_transform(df[categorical_cols])\n",
    "encoded_df = pd.DataFrame(encoded_categorical, columns=encoder.get_feature_names_out(categorical_cols))\n",
    "\n",
    "# Combine\n",
    "df_encoded = pd.concat([df[numerical_cols], encoded_df], axis=1)\n",
    "\n",
    "# Split X and y\n",
    "X = df_encoded.drop(columns=[\"listingPrice\"])\n",
    "y = df_encoded[\"listingPrice\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- MLflow Tracking ---\n",
    "with mlflow.start_run(run_name=\"Find Best Model\") as parent_run:\n",
    "    mlflow.set_tag(\"experiment_type\", \"All ML Models\")\n",
    "    mlflow.log_param(\"test_size\", 0.2)\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        with mlflow.start_run(run_name=model_name, nested=True):\n",
    "            mlflow.log_param(\"model\", model_name)\n",
    "\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            mae = mean_absolute_error(y_test, y_pred)\n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "            #log metrics\n",
    "            mlflow.log_metric(\"mae\", mae)\n",
    "            mlflow.log_metric(\"mse\", mse)\n",
    "            mlflow.log_metric(\"r2_score\", r2)\n",
    "\n",
    "            #log model\n",
    "            mlflow.sklearn.log_model(model, f\"{model_name}_model\")\n",
    "            \n",
    "            #log code file\n",
    "            mlflow.log_artifact(\"experiments.ipynb\")\n",
    "\n",
    "            print(f\"\\nModel: {model_name}\")\n",
    "            print(f\"  MAE: {mae:.2f}, MSE: {mse:.2f}, R2: {r2:.2f}\")\n",
    "            print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e441e45f",
   "metadata": {},
   "source": [
    "### Exp2: Hyperparameter Tuning\n",
    "\n",
    "- Tune hyper parameters of the best model (lowest MAE & highest R2) -- XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38dfaea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1: R2=0.3990, Params={'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.01, 'random_state': 42, 'verbosity': 0}\n",
      "Run 2: R2=0.7294, Params={'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.1, 'random_state': 42, 'verbosity': 0}\n",
      "Run 3: R2=0.7559, Params={'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.2, 'random_state': 42, 'verbosity': 0}\n",
      "Run 4: R2=0.5162, Params={'n_estimators': 100, 'max_depth': 6, 'learning_rate': 0.01, 'random_state': 42, 'verbosity': 0}\n",
      "Run 5: R2=0.7903, Params={'n_estimators': 100, 'max_depth': 6, 'learning_rate': 0.1, 'random_state': 42, 'verbosity': 0}\n",
      "Run 6: R2=0.8377, Params={'n_estimators': 100, 'max_depth': 6, 'learning_rate': 0.2, 'random_state': 42, 'verbosity': 0}\n",
      "Run 7: R2=0.5887, Params={'n_estimators': 100, 'max_depth': 10, 'learning_rate': 0.01, 'random_state': 42, 'verbosity': 0}\n",
      "Run 8: R2=0.8239, Params={'n_estimators': 100, 'max_depth': 10, 'learning_rate': 0.1, 'random_state': 42, 'verbosity': 0}\n",
      "Run 9: R2=0.8290, Params={'n_estimators': 100, 'max_depth': 10, 'learning_rate': 0.2, 'random_state': 42, 'verbosity': 0}\n",
      "Run 10: R2=0.5410, Params={'n_estimators': 200, 'max_depth': 3, 'learning_rate': 0.01, 'random_state': 42, 'verbosity': 0}\n",
      "Run 11: R2=0.7619, Params={'n_estimators': 200, 'max_depth': 3, 'learning_rate': 0.1, 'random_state': 42, 'verbosity': 0}\n",
      "Run 12: R2=0.7810, Params={'n_estimators': 200, 'max_depth': 3, 'learning_rate': 0.2, 'random_state': 42, 'verbosity': 0}\n",
      "Run 13: R2=0.6592, Params={'n_estimators': 200, 'max_depth': 6, 'learning_rate': 0.01, 'random_state': 42, 'verbosity': 0}\n",
      "Run 14: R2=0.8196, Params={'n_estimators': 200, 'max_depth': 6, 'learning_rate': 0.1, 'random_state': 42, 'verbosity': 0}\n",
      "Run 15: R2=0.8555, Params={'n_estimators': 200, 'max_depth': 6, 'learning_rate': 0.2, 'random_state': 42, 'verbosity': 0}\n",
      "Run 16: R2=0.7275, Params={'n_estimators': 200, 'max_depth': 10, 'learning_rate': 0.01, 'random_state': 42, 'verbosity': 0}\n",
      "Run 17: R2=0.8349, Params={'n_estimators': 200, 'max_depth': 10, 'learning_rate': 0.1, 'random_state': 42, 'verbosity': 0}\n",
      "Run 18: R2=0.8324, Params={'n_estimators': 200, 'max_depth': 10, 'learning_rate': 0.2, 'random_state': 42, 'verbosity': 0}\n",
      "Run 19: R2=0.6196, Params={'n_estimators': 300, 'max_depth': 3, 'learning_rate': 0.01, 'random_state': 42, 'verbosity': 0}\n",
      "Run 20: R2=0.7753, Params={'n_estimators': 300, 'max_depth': 3, 'learning_rate': 0.1, 'random_state': 42, 'verbosity': 0}\n",
      "Run 21: R2=0.7927, Params={'n_estimators': 300, 'max_depth': 3, 'learning_rate': 0.2, 'random_state': 42, 'verbosity': 0}\n",
      "Run 22: R2=0.7110, Params={'n_estimators': 300, 'max_depth': 6, 'learning_rate': 0.01, 'random_state': 42, 'verbosity': 0}\n",
      "Run 23: R2=0.8310, Params={'n_estimators': 300, 'max_depth': 6, 'learning_rate': 0.1, 'random_state': 42, 'verbosity': 0}\n",
      "Run 24: R2=0.8595, Params={'n_estimators': 300, 'max_depth': 6, 'learning_rate': 0.2, 'random_state': 42, 'verbosity': 0}\n",
      "Run 25: R2=0.7728, Params={'n_estimators': 300, 'max_depth': 10, 'learning_rate': 0.01, 'random_state': 42, 'verbosity': 0}\n",
      "Run 26: R2=0.8382, Params={'n_estimators': 300, 'max_depth': 10, 'learning_rate': 0.1, 'random_state': 42, 'verbosity': 0}\n",
      "Run 27: R2=0.8329, Params={'n_estimators': 300, 'max_depth': 10, 'learning_rate': 0.2, 'random_state': 42, 'verbosity': 0}\n",
      "\n",
      "Best Model R2: 0.8595, Best Params: {'n_estimators': 300, 'max_depth': 6, 'learning_rate': 0.2, 'random_state': 42, 'verbosity': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# --- Define parameter grid ---\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 6, 10],\n",
    "    'learning_rate': [0.01, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "# --- MLflow Parent Run ---\n",
    "with mlflow.start_run(run_name=\"XGBoost_Hyperparameter_Tuning\") as parent_run:\n",
    "    mlflow.set_tag(\"experiment_type\", \"xgboost_hyperparameter_tuning\")\n",
    "\n",
    "    best_r2 = -np.inf\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "\n",
    "    run_counter = 1\n",
    "\n",
    "    for n_estimators in param_grid['n_estimators']:\n",
    "        for max_depth in param_grid['max_depth']:\n",
    "            for learning_rate in param_grid['learning_rate']:\n",
    "                params = {\n",
    "                    'n_estimators': n_estimators,\n",
    "                    'max_depth': max_depth,\n",
    "                    'learning_rate': learning_rate,\n",
    "                    'random_state': 42,\n",
    "                    'verbosity': 0\n",
    "                }\n",
    "\n",
    "                with mlflow.start_run(run_name=f\"run_{run_counter}\", nested=True):\n",
    "                    mlflow.log_params(params)\n",
    "\n",
    "                    model = XGBRegressor(**params)\n",
    "                    model.fit(X_train, y_train)\n",
    "                    y_pred = model.predict(X_test)\n",
    "\n",
    "                    mae = mean_absolute_error(y_test, y_pred)\n",
    "                    mse = mean_squared_error(y_test, y_pred)\n",
    "                    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "                    mlflow.log_metrics({\n",
    "                        \"mae\": mae,\n",
    "                        \"mse\": mse,\n",
    "                        \"r2_score\": r2\n",
    "                    })\n",
    "\n",
    "                    mlflow.sklearn.log_model(model, \"model\")\n",
    "                    \n",
    "                    mlflow.log_artifact(\"experiments.ipynb\")\n",
    "\n",
    "                    print(f\"Run {run_counter}: R2={r2:.4f}, Params={params}\")\n",
    "\n",
    "                    if r2 > best_r2:\n",
    "                        best_r2 = r2\n",
    "                        best_params = params\n",
    "                        best_model = model\n",
    "\n",
    "                    run_counter += 1\n",
    "\n",
    "    # Log best model in parent run\n",
    "    mlflow.log_params({f\"best_{k}\": v for k, v in best_params.items()})\n",
    "    mlflow.log_metric(\"best_r2_score\", best_r2)\n",
    "    mlflow.sklearn.log_model(best_model, \"best_xgboost_model\")\n",
    "\n",
    "    print(f\"\\nBest Model R2: {best_r2:.4f}, Best Params: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72359ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dvc pipeline of xgboost model - with Params: {'n_estimators': 300, 'max_depth': 6, 'learning_rate': 0.2, 'random_state': 42, 'verbosity': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0b2c12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.8.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
